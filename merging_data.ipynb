{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging jan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Claim Date    Employee Name Employee ID        Designation  \\\n",
      "0  18-01-2024 00:00   Alexander Cole        2080          Associate   \n",
      "1  15-01-2024 00:00   Miles Martinez        2017  Software Engineer   \n",
      "2  16-01-2024 00:00  Felicity Taylor        2078         Associate    \n",
      "3  19-01-2024 00:00  Savannah Taylor        1870    Engagement Lead   \n",
      "4  17-01-2024 00:00     Mabel Gibson        2008  Software Engineer   \n",
      "\n",
      "  Project Name            Email address  Total Persons  Allowable Amount  \\\n",
      "0          NaN   Alexander.Cole@xyz.com              1               400   \n",
      "1    Greenmath   Miles.Martinez@xyz.com             19              7600   \n",
      "2          GPC  Felicity.Taylor@xyz.com              1               400   \n",
      "3  edgeCore.ai  Savannah.Taylor@xyz.com             10              4000   \n",
      "4    greenMath     Mabel.Gibson@xyz.com              4              1600   \n",
      "\n",
      "   Total Bill Amount                          List of Persons (as JSON)  \\\n",
      "0                321  [{\"employee_code\": \"2080\", \"name\": \"Alexander ...   \n",
      "1               7647  [{\"employee_code\": \"2005\", \"name\": \"Zoe Sulliv...   \n",
      "2                380  [{\"employee_code\": \"2078\", \"name\": \"Felicity T...   \n",
      "3               3941  [{\"employee_code\": \"1870\", \"name\": \"Savannah T...   \n",
      "4               1700  [{\"employee_code\": \"2008\", \"name\": \"Mabel Gibs...   \n",
      "\n",
      "                                                Bill     Seating Location  \n",
      "0  [{\"Bill_Date\": \"2024-01-03\", \"Bill_No\": \"01_12...  Workafella, Chennai  \n",
      "1  [{\"Bill_Date\": \"2024-01-02\", \"Bill_No\": \"01_37...            Bangalore  \n",
      "2  [{\"Bill_Date\": \"2024-01-03\", \"Bill_No\": \"01_16...  Workafella, Chennai  \n",
      "3  [{\"Bill_Date\": \"2024-01-17\", \"Bill_No\": \"01_14...  Workafella, Chennai  \n",
      "4  [{\"Bill_Date\": \"2024-01-06\", \"Bill_No\": \"01_55...  Workafella, Chennai  \n",
      "Number of matched Employee IDs: 653\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "df1 = pd.read_csv(\"Reimbursement(Jan-Sep).csv\")\n",
    "df2 = pd.read_csv(\"employee_details.csv\")\n",
    "\n",
    "# Rename \"Employee\" column to \"Employee ID\" in df2\n",
    "df2.rename(columns={\"Employee\": \"Employee ID\"}, inplace=True)\n",
    "\n",
    "# Standardize Employee ID formatting (strip spaces)\n",
    "df1[\"Employee ID\"] = df1[\"Employee ID\"].astype(str).str.strip()\n",
    "df2[\"Employee ID\"] = df2[\"Employee ID\"].astype(str).str.strip()\n",
    "\n",
    "# Convert Employee ID to consistent format (handling non-numeric values)\n",
    "df1[\"Employee ID\"] = pd.to_numeric(df1[\"Employee ID\"], errors='coerce')  # convert to numeric, set errors to NaN\n",
    "df1[\"Employee ID\"] = df1[\"Employee ID\"].fillna(0).astype(\"Int64\").astype(str)  # fill NaNs with 0 and convert to string\n",
    "\n",
    "df2[\"Employee ID\"] = pd.to_numeric(df2[\"Employee ID\"], errors='coerce')  # same for df2\n",
    "df2[\"Employee ID\"] = df2[\"Employee ID\"].fillna(0).astype(\"Int64\").astype(str)  # fill NaNs with 0 and convert to string\n",
    "\n",
    "# Merge on Employee ID\n",
    "df_merged = df1.merge(df2[[\"Employee ID\", \"Seating Location\"]], on=\"Employee ID\", how=\"left\")\n",
    "\n",
    "# Save the updated dataset\n",
    "df_merged.to_csv(\"merged_data_jan_sep.csv\", index=False)\n",
    "\n",
    "# Debugging info\n",
    "print(df_merged.head())\n",
    "\n",
    "# Check how many matched Employee IDs\n",
    "matched = df_merged[\"Seating Location\"].notna().sum()\n",
    "print(f\"Number of matched Employee IDs: {matched}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Claim Date    Employee Name Employee ID            Designation  \\\n",
      "0  21-10-2024 00:00       Abel Lopez        1959         People Partner   \n",
      "1  21-10-2024 00:00  Savannah Taylor        1870  Senior Data Scientist   \n",
      "2  10-10-2024 00:00  Savannah Taylor        1870  Senior Data Scientist   \n",
      "3  10-10-2024 00:00  Savannah Taylor        1870  Senior Data Scientist   \n",
      "4  10-10-2024 00:00  Savannah Taylor        1870  Senior Data Scientist   \n",
      "\n",
      "  Project Name            Email address  Total Persons  Allowable Amount  \\\n",
      "0           HR       Abel.Lopez@xyz.com              2               800   \n",
      "1  edgeCore.ai  Savannah.Taylor@xyz.com              6              2400   \n",
      "2  edgeCore.ai  Savannah.Taylor@xyz.com              2               800   \n",
      "3  edgeCore.ai  Savannah.Taylor@xyz.com              2               800   \n",
      "4  edgeCore.ai  Savannah.Taylor@xyz.com              2               800   \n",
      "\n",
      "   Total Bill Amount                          List of Persons (as JSON)  \\\n",
      "0              777.0  [{\"employee_code\": \"1959\", \"name\": \"Abel Lopez...   \n",
      "1             2588.0  [{\"employee_code\": \"1870\", \"name\": \"Savannah T...   \n",
      "2              882.0  [{\"employee_code\": \"1870\", \"name\": \"Savannah T...   \n",
      "3              882.0  [{\"employee_code\": \"1870\", \"name\": \"Savannah T...   \n",
      "4              882.0  [{\"employee_code\": \"1870\", \"name\": \"Savannah T...   \n",
      "\n",
      "                                                Bill     Seating Location  \n",
      "0  [{\"Bill_Date\": \"2024-10-10 00:00:00\", \"Bill_No...  Workafella, Chennai  \n",
      "1  [{\"Bill_Date\": \"2024-10-18 00:00:00\", \"Bill_No...  Workafella, Chennai  \n",
      "2  [{\"Bill_Date\": \"2024-10-04 00:00:00\", \"Bill_No...  Workafella, Chennai  \n",
      "3  [{\"Bill_Date\": \"2024-10-04 00:00:00\", \"Bill_No...  Workafella, Chennai  \n",
      "4  [{\"Bill_Date\": \"2024-10-04 00:00:00\", \"Bill_No...  Workafella, Chennai  \n",
      "Number of matched Employee IDs: 39\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "df1 = pd.read_csv(\"Reimbursement(Oct).csv\")\n",
    "df2 = pd.read_csv(\"employee_details.csv\")\n",
    "\n",
    "# Rename \"Employee\" column to \"Employee ID\" in df2\n",
    "df2.rename(columns={\"Employee\": \"Employee ID\"}, inplace=True)\n",
    "\n",
    "# Standardize Employee ID formatting (strip spaces)\n",
    "df1[\"Employee ID\"] = df1[\"Employee ID\"].astype(str).str.strip()\n",
    "df2[\"Employee ID\"] = df2[\"Employee ID\"].astype(str).str.strip()\n",
    "\n",
    "# Convert Employee ID to consistent format (handling non-numeric values)\n",
    "df1[\"Employee ID\"] = pd.to_numeric(df1[\"Employee ID\"], errors='coerce')  # convert to numeric, set errors to NaN\n",
    "df1[\"Employee ID\"] = df1[\"Employee ID\"].fillna(0).astype(\"Int64\").astype(str)  # fill NaNs with 0 and convert to string\n",
    "\n",
    "df2[\"Employee ID\"] = pd.to_numeric(df2[\"Employee ID\"], errors='coerce')  # same for df2\n",
    "df2[\"Employee ID\"] = df2[\"Employee ID\"].fillna(0).astype(\"Int64\").astype(str)  # fill NaNs with 0 and convert to string\n",
    "\n",
    "# Merge on Employee ID\n",
    "df_merged = df1.merge(df2[[\"Employee ID\", \"Seating Location\"]], on=\"Employee ID\", how=\"left\")\n",
    "\n",
    "# Save the updated dataset\n",
    "df_merged.to_csv(\"merged_data_oct.csv\", index=False)\n",
    "\n",
    "# Debugging info\n",
    "print(df_merged.head())\n",
    "\n",
    "# Check how many matched Employee IDs\n",
    "matched = df_merged[\"Seating Location\"].notna().sum()\n",
    "print(f\"Number of matched Employee IDs: {matched}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Claim Date    Employee Name Employee ID      Designation  \\\n",
      "0  21-11-2024 00:00  Savannah Taylor        1870  Engagement Lead   \n",
      "1  21-11-2024 00:00  Savannah Taylor        1870  Engagement Lead   \n",
      "2  30-11-2024 00:00  Savannah Taylor        1870  Engagement Lead   \n",
      "3  30-11-2024 00:00  Savannah Taylor        1870  Engagement Lead   \n",
      "4        20-11-2024       Nia Thomas        2068    Data Engineer   \n",
      "\n",
      "         Project Name            Email address  Total Persons  \\\n",
      "0         edgeCore.ai  Savannah.Taylor@xyz.com              3   \n",
      "1         edgeCore.ai  Savannah.Taylor@xyz.com              3   \n",
      "2  edgeCore.ai / Bose  Savannah.Taylor@xyz.com              8   \n",
      "3  edgeCore.ai / Bose  Savannah.Taylor@xyz.com              8   \n",
      "4                   -       Nia.Thomas@xyz.com              5   \n",
      "\n",
      "   Allowable Amount Total Bill Amount  \\\n",
      "0              1200              2153   \n",
      "1              1200              2153   \n",
      "2              3200              6033   \n",
      "3              3200              6033   \n",
      "4              2000              2160   \n",
      "\n",
      "                           List of Persons (as JSON)  \\\n",
      "0  [{\"employee_code\": \"1870\", \"name\": \"Savannah T...   \n",
      "1  [{\"employee_code\": \"1870\", \"name\": \"Savannah T...   \n",
      "2  [{\"employee_code\": \"1870\", \"name\": \"Savannah T...   \n",
      "3  [{\"employee_code\": \"1870\", \"name\": \"Savannah T...   \n",
      "4  [{\"employee_code\": \"2068\", \"name\": \"Nia Thomas...   \n",
      "\n",
      "                                                Bill     Seating Location  \n",
      "0  [{\"Bill_Date\": \"2024-11-01 00:00:00\", \"Bill_No...  Workafella, Chennai  \n",
      "1  [{\"Bill_Date\": \"2024-11-01 00:00:00\", \"Bill_No...  Workafella, Chennai  \n",
      "2  [{\"Bill_Date\": \"2024-11-22 00:00:00\", \"Bill_No...  Workafella, Chennai  \n",
      "3  [{\"Bill_Date\": \"2024-11-22 00:00:00\", \"Bill_No...  Workafella, Chennai  \n",
      "4  [{\"Bill_Date\": \"15-11-2024\", \"Bill_No\": 746142...  Workafella, Chennai  \n",
      "Number of matched Employee IDs: 56\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "df1 = pd.read_csv(\"Reimbursement(Nov).csv\")\n",
    "df2 = pd.read_csv(\"employee_details.csv\")\n",
    "\n",
    "# Rename \"Employee\" column to \"Employee ID\" in df2\n",
    "df2.rename(columns={\"Employee\": \"Employee ID\"}, inplace=True)\n",
    "\n",
    "# Standardize Employee ID formatting (strip spaces)\n",
    "df1[\"Employee ID\"] = df1[\"Employee ID\"].astype(str).str.strip()\n",
    "df2[\"Employee ID\"] = df2[\"Employee ID\"].astype(str).str.strip()\n",
    "\n",
    "# Convert Employee ID to consistent format (handling non-numeric values)\n",
    "df1[\"Employee ID\"] = pd.to_numeric(df1[\"Employee ID\"], errors='coerce')  # convert to numeric, set errors to NaN\n",
    "df1[\"Employee ID\"] = df1[\"Employee ID\"].fillna(0).astype(\"Int64\").astype(str)  # fill NaNs with 0 and convert to string\n",
    "\n",
    "df2[\"Employee ID\"] = pd.to_numeric(df2[\"Employee ID\"], errors='coerce')  # same for df2\n",
    "df2[\"Employee ID\"] = df2[\"Employee ID\"].fillna(0).astype(\"Int64\").astype(str)  # fill NaNs with 0 and convert to string\n",
    "\n",
    "# Merge on Employee ID\n",
    "df_merged = df1.merge(df2[[\"Employee ID\", \"Seating Location\"]], on=\"Employee ID\", how=\"left\")\n",
    "\n",
    "# Save the updated dataset\n",
    "df_merged.to_csv(\"merged_data_nov.csv\", index=False)\n",
    "\n",
    "# Debugging info\n",
    "print(df_merged.head())\n",
    "\n",
    "# Check how many matched Employee IDs\n",
    "matched = df_merged[\"Seating Location\"].notna().sum()\n",
    "print(f\"Number of matched Employee IDs: {matched}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Claim Date    Employee Name Employee ID      Designation  \\\n",
      "0  03-01-2025 00:00       Luca Allen        1957              STL   \n",
      "1  03-01-2025 00:00       Luca Allen        1957              STL   \n",
      "2  03-01-2025 00:00       Luca Allen        1957              STL   \n",
      "3  03-01-2025 00:00       Luca Allen        1957              STL   \n",
      "4  30-12-2024 00:00  Savannah Taylor        1870  Engagement Lead   \n",
      "\n",
      "          Project Name            Email address  Total Persons  \\\n",
      "0  edgeCore, GreenMath       Luca.Allen@xyz.com             10   \n",
      "1  edgeCore, GreenMath       Luca.Allen@xyz.com             10   \n",
      "2  edgeCore, GreenMath       Luca.Allen@xyz.com             10   \n",
      "3  edgeCore, GreenMath       Luca.Allen@xyz.com             10   \n",
      "4          edgeCore.ai  Savannah.Taylor@xyz.com             11   \n",
      "\n",
      "   Allowable Amount  Total Bill Amount  \\\n",
      "0              4000             4863.0   \n",
      "1              4000             4863.0   \n",
      "2              4000             4863.0   \n",
      "3              4000             4863.0   \n",
      "4              4400             5506.0   \n",
      "\n",
      "                           List of Persons (as JSON)  \\\n",
      "0  [{\"employee_code\": \"1957\", \"name\": \"Luca Allen...   \n",
      "1  [{\"employee_code\": \"1957\", \"name\": \"Luca Allen...   \n",
      "2  [{\"employee_code\": \"1957\", \"name\": \"Luca Allen...   \n",
      "3  [{\"employee_code\": \"1957\", \"name\": \"Luca Allen...   \n",
      "4  [{\"employee_code\": \"1870\", \"name\": \"Savannah T...   \n",
      "\n",
      "                                                Bill     Seating Location  \n",
      "0  [{\"Bill_Date\": \"2025-01-03 00:00:00\", \"Bill_No...     Bhive, Bangalore  \n",
      "1  [{\"Bill_Date\": \"2025-01-03 00:00:00\", \"Bill_No...     Bhive, Bangalore  \n",
      "2  [{\"Bill_Date\": \"2025-01-03 00:00:00\", \"Bill_No...     Bhive, Bangalore  \n",
      "3  [{\"Bill_Date\": \"2025-01-03 00:00:00\", \"Bill_No...     Bhive, Bangalore  \n",
      "4  [{\"Bill_Date\": \"2024-12-27 00:00:00\", \"Bill_No...  Workafella, Chennai  \n",
      "Number of matched Employee IDs: 38\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "df1 = pd.read_csv(\"Reimbursement(Dec).csv\")\n",
    "df2 = pd.read_csv(\"employee_details.csv\")\n",
    "\n",
    "# Rename \"Employee\" column to \"Employee ID\" in df2\n",
    "df2.rename(columns={\"Employee\": \"Employee ID\"}, inplace=True)\n",
    "\n",
    "# Standardize Employee ID formatting (strip spaces)\n",
    "df1[\"Employee ID\"] = df1[\"Employee ID\"].astype(str).str.strip()\n",
    "df2[\"Employee ID\"] = df2[\"Employee ID\"].astype(str).str.strip()\n",
    "\n",
    "# Convert Employee ID to consistent format (handling non-numeric values)\n",
    "df1[\"Employee ID\"] = pd.to_numeric(df1[\"Employee ID\"], errors='coerce')  # convert to numeric, set errors to NaN\n",
    "df1[\"Employee ID\"] = df1[\"Employee ID\"].fillna(0).astype(\"Int64\").astype(str)  # fill NaNs with 0 and convert to string\n",
    "\n",
    "df2[\"Employee ID\"] = pd.to_numeric(df2[\"Employee ID\"], errors='coerce')  # same for df2\n",
    "df2[\"Employee ID\"] = df2[\"Employee ID\"].fillna(0).astype(\"Int64\").astype(str)  # fill NaNs with 0 and convert to string\n",
    "\n",
    "# Merge on Employee ID\n",
    "df_merged = df1.merge(df2[[\"Employee ID\", \"Seating Location\"]], on=\"Employee ID\", how=\"left\")\n",
    "\n",
    "# Save the updated dataset\n",
    "df_merged.to_csv(\"merged_data_dec.csv\", index=False)\n",
    "\n",
    "# Debugging info\n",
    "print(df_merged.head())\n",
    "\n",
    "# Check how many matched Employee IDs\n",
    "matched = df_merged[\"Seating Location\"].notna().sum()\n",
    "print(f\"Number of matched Employee IDs: {matched}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the merged datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Claim Date    Employee Name  Employee ID        Designation  \\\n",
      "0  18-01-2024 00:00   Alexander Cole         2080          Associate   \n",
      "1  15-01-2024 00:00   Miles Martinez         2017  Software Engineer   \n",
      "2  16-01-2024 00:00  Felicity Taylor         2078         Associate    \n",
      "3  19-01-2024 00:00  Savannah Taylor         1870    Engagement Lead   \n",
      "4  17-01-2024 00:00     Mabel Gibson         2008  Software Engineer   \n",
      "\n",
      "  Project Name            Email address  Total Persons  Allowable Amount  \\\n",
      "0          NaN   Alexander.Cole@xyz.com              1               400   \n",
      "1    Greenmath   Miles.Martinez@xyz.com             19              7600   \n",
      "2          GPC  Felicity.Taylor@xyz.com              1               400   \n",
      "3  edgeCore.ai  Savannah.Taylor@xyz.com             10              4000   \n",
      "4    greenMath     Mabel.Gibson@xyz.com              4              1600   \n",
      "\n",
      "  Total Bill Amount                          List of Persons (as JSON)  \\\n",
      "0               321  [{\"employee_code\": \"2080\", \"name\": \"Alexander ...   \n",
      "1              7647  [{\"employee_code\": \"2005\", \"name\": \"Zoe Sulliv...   \n",
      "2               380  [{\"employee_code\": \"2078\", \"name\": \"Felicity T...   \n",
      "3              3941  [{\"employee_code\": \"1870\", \"name\": \"Savannah T...   \n",
      "4              1700  [{\"employee_code\": \"2008\", \"name\": \"Mabel Gibs...   \n",
      "\n",
      "                                                Bill     Seating Location  \n",
      "0  [{\"Bill_Date\": \"2024-01-03\", \"Bill_No\": \"01_12...  Workafella, Chennai  \n",
      "1  [{\"Bill_Date\": \"2024-01-02\", \"Bill_No\": \"01_37...            Bangalore  \n",
      "2  [{\"Bill_Date\": \"2024-01-03\", \"Bill_No\": \"01_16...  Workafella, Chennai  \n",
      "3  [{\"Bill_Date\": \"2024-01-17\", \"Bill_No\": \"01_14...  Workafella, Chennai  \n",
      "4  [{\"Bill_Date\": \"2024-01-06\", \"Bill_No\": \"01_55...  Workafella, Chennai  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your datasets for each month\n",
    "df_jan = pd.read_csv(\"merged_data_jan_sep.csv\")  # January data\n",
    "df_oct = pd.read_csv(\"merged_data_oct.csv\")  # October data\n",
    "df_nov = pd.read_csv(\"merged_data_nov.csv\")  # November data\n",
    "df_dec = pd.read_csv(\"merged_data_dec.csv\")  # December data\n",
    "\n",
    "# List all datasets to combine\n",
    "dfs = [df_jan, df_oct, df_nov, df_dec]\n",
    "\n",
    "# Concatenate datasets vertically (row-wise)\n",
    "df_combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "df_combined.to_csv(\"Reimbursement_combined.csv\", index=False)\n",
    "\n",
    "# Display the combined dataset (for verification)\n",
    "print(df_combined.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted the Json values in the Bill column based on the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as transformed_Bill.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"Reimbursement_combined.csv\")  # Replace with your actual filename\n",
    "\n",
    "\n",
    "# Convert 'Bill' column from string to a list of dictionaries\n",
    "df[\"Bill\"] = df[\"Bill\"].apply(lambda x: json.loads(x)[0])  # Extract first dictionary\n",
    "\n",
    "# Expand JSON columns\n",
    "df_expanded = df[\"Bill\"].apply(pd.Series)\n",
    "\n",
    "# Merge with original data (if needed) and remove the original 'Bill' column\n",
    "df_final = df.drop(columns=[\"Bill\"]).join(df_expanded)\n",
    "\n",
    "# Save to a new CSV file\n",
    "df_final.to_csv(\"transformed_Bill.csv\", index=False)\n",
    "\n",
    "print(\"File saved as transformed_Bill.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seating location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"transformed_Bill.csv\")\n",
    "df['Seating Location'] = df['Seating Location'].replace('Workafella, Chennai', 'Chennai')\n",
    "df['Seating Location'] = df['Seating Location'].replace('Bhive, Bangalore', 'Bangalore')\n",
    "df.to_csv(\"transformed_Bill.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Claim Date    Employee Name  Employee ID        Designation  \\\n",
      "0  18-01-2024 00:00   Alexander Cole         2080          Associate   \n",
      "1  15-01-2024 00:00   Miles Martinez         2017  Software Engineer   \n",
      "2  16-01-2024 00:00  Felicity Taylor         2078         Associate    \n",
      "3  19-01-2024 00:00  Savannah Taylor         1870    Engagement Lead   \n",
      "4  17-01-2024 00:00     Mabel Gibson         2008  Software Engineer   \n",
      "\n",
      "  Project Name            Email address  Total Persons  Allowable Amount  \\\n",
      "0          NaN   Alexander.Cole@xyz.com              1               400   \n",
      "1    Greenmath   Miles.Martinez@xyz.com             19              7600   \n",
      "2          GPC  Felicity.Taylor@xyz.com              1               400   \n",
      "3  edgeCore.ai  Savannah.Taylor@xyz.com             10              4000   \n",
      "4    greenMath     Mabel.Gibson@xyz.com              4              1600   \n",
      "\n",
      "  Total Bill Amount                          List of Persons (as JSON)  \\\n",
      "0               321  [{\"employee_code\": \"2080\", \"name\": \"Alexander ...   \n",
      "1              7647  [{\"employee_code\": \"2005\", \"name\": \"Zoe Sulliv...   \n",
      "2               380  [{\"employee_code\": \"2078\", \"name\": \"Felicity T...   \n",
      "3              3941  [{\"employee_code\": \"1870\", \"name\": \"Savannah T...   \n",
      "4              1700  [{\"employee_code\": \"2008\", \"name\": \"Mabel Gibs...   \n",
      "\n",
      "  Seating Location   Bill_Date Bill_No Total_Price  \n",
      "0          Chennai  2024-01-03  01_124         321  \n",
      "1        Bangalore  2024-01-02   01_37        7647  \n",
      "2          Chennai  2024-01-03  01_163         380  \n",
      "3          Chennai  2024-01-17  01_142        3941  \n",
      "4          Chennai  2024-01-06   01_55        1700  \n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Error parsing JSON: 'float' object has no attribute 'replace'\n",
      "Total Entries: 13555\n",
      "Valid Employee Codes: 4879\n",
      "Valid Names: 13073\n",
      "Total Unique Employees: 93\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"transformed_Bill.csv\"  # Update with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming the JSON is inside a column named 'persons_data'\n",
    "json_column = \"List of Persons (as JSON)\"  # Replace with the actual column name\n",
    "\n",
    "# Function to extract employee_code and name from JSON column\n",
    "def extract_employee_details(json_str):\n",
    "    try:\n",
    "        # Parse JSON string into a Python list of dictionaries\n",
    "        employees = json.loads(json_str.replace(\"'\", \"\\\"\"))  # Handle single quotes if needed\n",
    "        \n",
    "        # Extract employee_code and name from each dictionary\n",
    "        return [(emp.get(\"employee_code\", None), emp.get(\"name\", None)) for emp in employees]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return []  # Return empty list if JSON parsing fails\n",
    "\n",
    "# Apply function and explode to create individual rows\n",
    "df[\"employee_details\"] = df[json_column].apply(extract_employee_details)\n",
    "df = df.explode(\"employee_details\")\n",
    "\n",
    "# Convert tuple columns into separate columns\n",
    "df[[\"employee_code\", \"name\"]] = pd.DataFrame(df[\"employee_details\"].tolist(), index=df.index)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=[\"employee_details\", json_column])\n",
    "\n",
    "# Count valid values\n",
    "valid_employee_code = df[\"employee_code\"].notna().sum()\n",
    "valid_name = df[\"name\"].notna().sum()\n",
    "\n",
    "print(f\"Total Entries: {len(df)}\")\n",
    "print(f\"Valid Employee Codes: {valid_employee_code}\")\n",
    "print(f\"Valid Names: {valid_name}\")\n",
    "\n",
    "# Count unique employees\n",
    "unique_employees = df.dropna(subset=[\"employee_code\", \"name\"]).drop_duplicates(subset=[\"employee_code\", \"name\"])\n",
    "print(f\"Total Unique Employees: {len(unique_employees)}\")\n",
    "\n",
    "# Save to CSV (optional)\n",
    "df.to_csv(\"processed_employees.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Inconsistencies found! The following Employee IDs have multiple names or designations:\n",
      "            Claim Date    Employee Name  Employee ID            Designation  \\\n",
      "3      19 January 2024  Savannah Taylor       1870.0        Engagement Lead   \n",
      "8      28 January 2024  Savannah Taylor       1870.0  Senior Data Scientist   \n",
      "11     25 January 2024  Savannah Taylor       1870.0  Senior Data Scientist   \n",
      "12     26 January 2024   Felicity Craig          0.0                 intern   \n",
      "13     26 January 2024   Felicity Craig          0.0                 intern   \n",
      "...                ...              ...          ...                    ...   \n",
      "1166  20 December 2024      Miles Lynch          0.0                 Intern   \n",
      "1167  20 December 2024      Miles Lynch          0.0                 Intern   \n",
      "1185  28 December 2024   Vivian Johnson       2029.0   Senior Data Engineer   \n",
      "1186  28 December 2024   Vivian Johnson       2029.0   Senior Data Engineer   \n",
      "1187  24 December 2024   Vivian Johnson       2029.0          Data Engineer   \n",
      "\n",
      "                Project Name            Email address  Total Persons  \\\n",
      "3                edgeCore.ai  Savannah.Taylor@xyz.com           10.0   \n",
      "8                edgeCore.ai  Savannah.Taylor@xyz.com            2.0   \n",
      "11               edgeCore.ai  Savannah.Taylor@xyz.com            6.0   \n",
      "12                        o9   Felicity.Craig@xyz.com            1.0   \n",
      "13                        o9   Felicity.Craig@xyz.com            1.0   \n",
      "...                      ...                      ...            ...   \n",
      "1166                 Traning      Miles.Lynch@xyz.com            2.0   \n",
      "1167                 Traning      Miles.Lynch@xyz.com            2.0   \n",
      "1185                      o9   Vivian.Johnson@xyz.com            7.0   \n",
      "1186                      o9   Vivian.Johnson@xyz.com            7.0   \n",
      "1187  o9 -  TMobile Resilinc   Vivian.Johnson@xyz.com            1.0   \n",
      "\n",
      "      Allowable Amount Total Bill Amount  \\\n",
      "3               4000.0              3941   \n",
      "8                800.0               896   \n",
      "11              2400.0              2392   \n",
      "12               400.0               379   \n",
      "13               400.0               379   \n",
      "...                ...               ...   \n",
      "1166             800.0              1021   \n",
      "1167             800.0              1021   \n",
      "1185            2800.0              3638   \n",
      "1186            2800.0              3638   \n",
      "1187             400.0               409   \n",
      "\n",
      "                              List of Persons (as JSON) Seating Location  \\\n",
      "3     [{\"employee_code\": \"1870\", \"name\": \"Savannah T...          Chennai   \n",
      "8     [{\"employee_code\": \"1870\", \"name\": \"Savannah T...          Chennai   \n",
      "11    [{\"employee_code\": \"1870\", \"name\": \"Savannah T...          Chennai   \n",
      "12    [{\"employee_code\": NaN, \"name\": \"Autumn Hughes...              NaN   \n",
      "13    [{\"employee_code\": NaN, \"name\": \"Autumn Hughes...              NaN   \n",
      "...                                                 ...              ...   \n",
      "1166  [{\"employee_code\": NaN, \"name\": NaN}, {\"employ...              NaN   \n",
      "1167  [{\"employee_code\": NaN, \"name\": NaN}, {\"employ...              NaN   \n",
      "1185  [{\"employee_code\": \"2029\", \"name\": \"Vivian Joh...          Chennai   \n",
      "1186  [{\"employee_code\": \"2029\", \"name\": \"Vivian Joh...          Chennai   \n",
      "1187  [{\"employee_code\": \"2029\", \"name\": \"Vivian Joh...          Chennai   \n",
      "\n",
      "             Bill_Date              Bill_No Total_Price  \n",
      "3           17-01-2024               01_142        3941  \n",
      "8           09-01-2024                 01_3         896  \n",
      "11          18-01-2024                 01_2        2392  \n",
      "12          09-01-2024                01_73         379  \n",
      "13          09-01-2024                01_73         379  \n",
      "...                ...                  ...         ...  \n",
      "1166        20-12-2024  2117201224024410000        1021  \n",
      "1167        20-12-2024  2117201224024410000        1021  \n",
      "1185  27-12-2024 00:00      20241227_165916        3638  \n",
      "1186  27-12-2024 00:00      20241227_165916        3638  \n",
      "1187  20-12-2024 00:00      193649043704089         409  \n",
      "\n",
      "[559 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "# Load the CSV file\n",
    "file_path = \"transformed_Bill.csv\"  # Update with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Group by employee_id and check unique combinations of (name, designation)\n",
    "duplicates = df.groupby(\"Employee ID\")[[\"Employee Name\", \"Designation\"]].nunique()\n",
    "\n",
    "# Identify Employee IDs where either Name or Designation is inconsistent\n",
    "inconsistent_ids = duplicates[(duplicates[\"Employee Name\"] > 1) | (duplicates[\"Designation\"] > 1)].index\n",
    "\n",
    "# Extract rows with inconsistencies\n",
    "inconsistent_rows = df[df[\"Employee ID\"].isin(inconsistent_ids)]\n",
    "\n",
    "# Display results\n",
    "if inconsistent_rows.empty:\n",
    "    print(\"✅ All Employee IDs consistently match the same Employee Name and Designation.\")\n",
    "else:\n",
    "    print(\"⚠️ Inconsistencies found! The following Employee IDs have multiple names or designations:\")\n",
    "    print(inconsistent_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Inconsistencies found! The affected records have been saved in 'inconsistent_employees.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"transformed_Bill.csv\"  # Update with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Group by employee_id and check unique counts for (name, designation)\n",
    "duplicates = df.groupby(\"Employee ID\")[[\"Employee Name\"]].nunique()\n",
    "\n",
    "# Identify Employee IDs where either Name or Designation is inconsistent\n",
    "inconsistent_ids = duplicates[(duplicates[\"Employee Name\"] > 1)].index\n",
    "\n",
    "# Extract inconsistent rows\n",
    "inconsistent_rows = df[df[\"Employee ID\"].isin(inconsistent_ids)]\n",
    "\n",
    "# Save the inconsistent records to a separate CSV file\n",
    "output_file = \"inconsistent_employees.csv\"\n",
    "inconsistent_rows.to_csv(output_file, index=False)\n",
    "\n",
    "# Display results\n",
    "if inconsistent_rows.empty:\n",
    "    print(\"✅ All Employee IDs consistently match the same Employee Name and Designation.\")\n",
    "else:\n",
    "    print(f\"⚠️ Inconsistencies found! The affected records have been saved in '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Reimbursement_cleaned_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load CSV file\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m cleaned_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReimbursement_cleaned_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Remove rows where ALL columns are blank\u001b[39;00m\n\u001b[0;32m      7\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m cleaned_df\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Reimbursement_cleaned_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "cleaned_df = pd.read_csv(\"Reimbursement_cleaned_data.csv\")\n",
    "\n",
    "# Remove rows where ALL columns are blank\n",
    "df_cleaned = cleaned_df.dropna(how='all')\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_cleaned.to_csv(\"Reimbursement_cleaned_data.csv\", index=False)\n",
    "\n",
    "print(\"Completely blank rows removed. Cleaned file saved as 'Reimbursement_Cleaned.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
